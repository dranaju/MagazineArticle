\section*{Conclusion}

In this paper was developed a neural network, DDPG, to be used on navigation of a mobile robot through continuous control in a virtual environment.
Thus, a deep reinforcement learning network algorithm was able to solve the problem of robot navigation.
It was proposed as a task that the robot could reach a target position in different simulated environments and it was created a reward function so that the DDPG network could give as results, the linear and angular velocity for the robot. All the network structure created was applied on the Gazebo simulation environments with success.

With the training results obtained on the simulation environments, it was analyzed the performance of the intelligent agent algorithm in the task of avoiding obstacles and getting to the final goal. 
On the two simulated environments proposed, the algorithm had a good performance.
% , however, in the last environment it was observed that sometimes even after many training episodes the mobile robot would still collide with some obstacle.

It is possible to conclude that DDPG networks are suitable for the development of applications that need a continuous control on the robotics.
The Deep-RL networks can produce excellent results if the reward system is well-made for the problem that it wants to solve.
It was proved that intelligent agents can move around in a complex simulated environments as well as in real world environments without any previous knowledge of the scenario.
The network based on deep deterministic policy gradients can provide means of unifying the machine learning for the control of robotic systems.
In a future work, we are planning to apply the DDPG network in a task that will have to control a a system with more than one agent.
% As future works, it is intended to use the deep reinforcement learning technique for the navigation of the Turtlebot3 Burger robot in a real environment, thus, being able to validate the training in a simulated environment in a real environment.