\section*{Conclusion}

In this paper was developed a neural network, DDPG, to be used on navigation of a mobile robot through continuous control in a virtual environment.
Thus achieving a deep reinforcement learning network structure able to solve the problem of robot navigation.
It was proposed as a task that the robot could reach a target position in different simulated environments and it created a reward function so that the DDPG network could give as results, the linear and angular velocity for the robot. All the network structure created was applied on the Gazebo simulation environments with success.

With the training results obtained on the simulation environments, it was analyzed the performance of the intelligent agent algorithm in the task of avoiding obstacles and getting to the final goal. 
On the three environments proposed, the algorithm had a good performance, however, in the last environment it was observed that sometimes even after many training episodes the mobile robot would still collide with some obstacle.

It is possible to conclude that DDPG networks are suitable for the development of applications that need a continuous control on the robotics.
The Deep-RL networks can produce excellent results if the reward system is well-made for the problem that it wants to solve.
It was proved that intelligent agents can move around in a complex simulated environment without any previous knowledge of the environment.
The network based on deep deterministic policy gradients can provide means of unifying the machine learning for the control of robotic systems.
This technique can be applied in function as manipulation of robotic arms, pendulums, games among others.
As future works, it is intended to use the deep reinforcement learning technique for the navigation of the TurtleBot3 Burger robot in a real environment, thus, being able to validate the training in a simulated environment in a real environment.